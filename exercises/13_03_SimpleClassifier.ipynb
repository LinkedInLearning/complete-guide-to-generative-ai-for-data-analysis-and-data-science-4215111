{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP6BbROcFol5emZ2F6bFoip"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","# Load the dataset\n","file_path = '/content/Synthetic_Iris_Like_Data - missing_synthetic_iris_like_data.csv'\n","data = pd.read_csv(file_path)\n","\n","# Split the data into features and labels\n","X = data.drop('class', axis=1)\n","y = data['class']\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Initialize and train the classifier\n","classifier = RandomForestClassifier(random_state=42)\n","classifier.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = classifier.predict(X_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","report = classification_report(y_test, y_pred)\n","\n","print(f'Accuracy: {accuracy:.2f}')\n","print('Classification Report:')\n","print(report)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495},"id":"-FICvXHGyVgX","executionInfo":{"status":"error","timestamp":1718470000872,"user_tz":420,"elapsed":367,"user":{"displayName":"Dan Sullivan","userId":"04950569771201204395"}},"outputId":"9a57bf8e-4aab-4f3b-9596-b2a7aaf6fd76"},"execution_count":4,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-a4fe43e93344>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Initialize and train the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         )\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1107\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","# Load the dataset\n","file_path = '/content/Synthetic_Iris_Like_Data - missing_synthetic_iris_like_data.csv'\n","data = pd.read_csv(file_path)\n","\n","# Function to fill missing values with the mean value of the column for the corresponding label\n","def fill_missing_with_label_mean(df, label_col):\n","    for col in df.columns:\n","        if col != label_col:\n","            df[col] = df.groupby(label_col)[col].transform(lambda x: x.fillna(x.mean()))\n","    return df\n","\n","# Fill missing values\n","data = fill_missing_with_label_mean(data, 'class')\n","\n","# Split the data into features and labels\n","X = data.drop('class', axis=1)\n","y = data['class']\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Initialize and train the classifier\n","classifier = RandomForestClassifier(random_state=42)\n","classifier.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = classifier.predict(X_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","report = classification_report(y_test, y_pred)\n","\n","print(f'Accuracy: {accuracy:.2f}')\n","print('Classification Report:')\n","print(report)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"niPyB9ARzLuE","executionInfo":{"status":"ok","timestamp":1718470231956,"user_tz":420,"elapsed":657,"user":{"displayName":"Dan Sullivan","userId":"04950569771201204395"}},"outputId":"b3079190-4320-44b7-be55-8fce45c7dc43"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.83\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.76      0.89      0.82        18\n","           1       0.85      0.77      0.81        22\n","           2       0.89      0.85      0.87        20\n","\n","    accuracy                           0.83        60\n","   macro avg       0.84      0.84      0.83        60\n","weighted avg       0.84      0.83      0.83        60\n","\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","# Load the dataset\n","file_path = '/content/Synthetic_Iris_Like_Data - missing_synthetic_iris_like_data.csv'\n","data = pd.read_csv(file_path)\n","\n","# Function to fill missing values with the mean value of the column for the corresponding label\n","def fill_missing_with_label_mean(df, label_col):\n","    for col in df.columns:\n","        if col != label_col:\n","            df[col] = df.groupby(label_col)[col].transform(lambda x: x.fillna(x.mean()))\n","    return df\n","\n","# Fill missing values\n","data = fill_missing_with_label_mean(data, 'class')\n","\n","# Split the data into features and labels\n","X = data.drop('class', axis=1)\n","y = data['class']\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Initialize and train the Random Forest classifier\n","rf_classifier = RandomForestClassifier(random_state=42)\n","rf_classifier.fit(X_train, y_train)\n","\n","# Make predictions with Random Forest\n","rf_y_pred = rf_classifier.predict(X_test)\n","\n","# Evaluate the Random Forest model\n","rf_accuracy = accuracy_score(y_test, rf_y_pred)\n","rf_report = classification_report(y_test, rf_y_pred)\n","\n","print(f'Random Forest Accuracy: {rf_accuracy:.2f}')\n","print('Random Forest Classification Report:')\n","print(rf_report)\n","\n","# Initialize and train the XGBoost classifier\n","xgb_classifier = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n","xgb_classifier.fit(X_train, y_train)\n","\n","# Make predictions with XGBoost\n","xgb_y_pred = xgb_classifier.predict(X_test)\n","\n","# Evaluate the XGBoost model\n","xgb_accuracy = accuracy_score(y_test, xgb_y_pred)\n","xgb_report = classification_report(y_test, xgb_y_pred)\n","\n","print(f'XGBoost Accuracy: {xgb_accuracy:.2f}')\n","print('XGBoost Classification Report:')\n","print(xgb_report)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YB7Or96o0tJe","executionInfo":{"status":"ok","timestamp":1718470621629,"user_tz":420,"elapsed":952,"user":{"displayName":"Dan Sullivan","userId":"04950569771201204395"}},"outputId":"1d7bef2f-7b88-47b5-9d5d-9b7c42eb1406"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Random Forest Accuracy: 0.83\n","Random Forest Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.76      0.89      0.82        18\n","           1       0.85      0.77      0.81        22\n","           2       0.89      0.85      0.87        20\n","\n","    accuracy                           0.83        60\n","   macro avg       0.84      0.84      0.83        60\n","weighted avg       0.84      0.83      0.83        60\n","\n","XGBoost Accuracy: 0.83\n","XGBoost Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.75      1.00      0.86        18\n","           1       0.89      0.73      0.80        22\n","           2       0.89      0.80      0.84        20\n","\n","    accuracy                           0.83        60\n","   macro avg       0.84      0.84      0.83        60\n","weighted avg       0.85      0.83      0.83        60\n","\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from sklearn.metrics import classification_report, accuracy_score\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.utils import to_categorical\n","\n","# Load the dataset\n","file_path = '/content/Synthetic_Iris_Like_Data - missing_synthetic_iris_like_data.csv'\n","data = pd.read_csv(file_path)\n","\n","# Function to fill missing values with the mean value of the column for the corresponding label\n","def fill_missing_with_label_mean(df, label_col):\n","    for col in df.columns:\n","        if col != label_col:\n","            df[col] = df.groupby(label_col)[col].transform(lambda x: x.fillna(x.mean()))\n","    return df\n","\n","# Fill missing values\n","data = fill_missing_with_label_mean(data, 'class')\n","\n","# Split the data into features and labels\n","X = data.drop('class', axis=1)\n","y = data['class']\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Initialize and train the Random Forest classifier\n","rf_classifier = RandomForestClassifier(random_state=42)\n","rf_classifier.fit(X_train, y_train)\n","\n","# Make predictions with Random Forest\n","rf_y_pred = rf_classifier.predict(X_test)\n","\n","# Evaluate the Random Forest model\n","rf_accuracy = accuracy_score(y_test, rf_y_pred)\n","rf_report = classification_report(y_test, rf_y_pred)\n","\n","print(f'Random Forest Accuracy: {rf_accuracy:.2f}')\n","print('Random Forest Classification Report:')\n","print(rf_report)\n","\n","# Initialize and train the XGBoost classifier\n","xgb_classifier = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n","xgb_classifier.fit(X_train, y_train)\n","\n","# Make predictions with XGBoost\n","xgb_y_pred = xgb_classifier.predict(X_test)\n","\n","# Evaluate the XGBoost model\n","xgb_accuracy = accuracy_score(y_test, xgb_y_pred)\n","xgb_report = classification_report(y_test, xgb_y_pred)\n","\n","print(f'XGBoost Accuracy: {xgb_accuracy:.2f}')\n","print('XGBoost Classification Report:')\n","print(xgb_report)\n","\n","# Convert labels to categorical for neural network\n","y_train_cat = to_categorical(y_train)\n","y_test_cat = to_categorical(y_test)\n","\n","# Initialize and train the neural network\n","nn_model = Sequential()\n","nn_model.add(Dense(10, input_shape=(X_train.shape[1],), activation='relu'))\n","nn_model.add(Dense(y_train_cat.shape[1], activation='softmax'))\n","\n","nn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","nn_model.fit(X_train, y_train_cat, epochs=50, batch_size=10, verbose=0)\n","\n","# Make predictions with the neural network\n","nn_y_pred_cat = nn_model.predict(X_test)\n","nn_y_pred = nn_y_pred_cat.argmax(axis=1)\n","\n","# Evaluate the neural network model\n","nn_accuracy = accuracy_score(y_test, nn_y_pred)\n","nn_report = classification_report(y_test, nn_y_pred)\n","\n","print(f'Neural Network Accuracy: {nn_accuracy:.2f}')\n","print('Neural Network Classification Report:')\n","print(nn_report)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5oXmSDN51pti","executionInfo":{"status":"ok","timestamp":1718470870004,"user_tz":420,"elapsed":3901,"user":{"displayName":"Dan Sullivan","userId":"04950569771201204395"}},"outputId":"af5d3812-ef45-43da-9b9d-79470a251aa9"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Random Forest Accuracy: 0.83\n","Random Forest Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.76      0.89      0.82        18\n","           1       0.85      0.77      0.81        22\n","           2       0.89      0.85      0.87        20\n","\n","    accuracy                           0.83        60\n","   macro avg       0.84      0.84      0.83        60\n","weighted avg       0.84      0.83      0.83        60\n","\n","XGBoost Accuracy: 0.83\n","XGBoost Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.75      1.00      0.86        18\n","           1       0.89      0.73      0.80        22\n","           2       0.89      0.80      0.84        20\n","\n","    accuracy                           0.83        60\n","   macro avg       0.84      0.84      0.83        60\n","weighted avg       0.85      0.83      0.83        60\n","\n","2/2 [==============================] - 0s 9ms/step\n","Neural Network Accuracy: 0.77\n","Neural Network Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.74      0.94      0.83        18\n","           1       0.74      0.64      0.68        22\n","           2       0.83      0.75      0.79        20\n","\n","    accuracy                           0.77        60\n","   macro avg       0.77      0.78      0.77        60\n","weighted avg       0.77      0.77      0.76        60\n","\n"]}]}]}